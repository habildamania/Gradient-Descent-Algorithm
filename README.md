# Gradient-Descent-Algorithm
Gradient Descent is an Optimization technique to Update the weight vectors in a classifier in order to reduce the error of classification. Theoretically, the gradient descent should terminate when there is convergence and the error becomes 0 on a set of weight vectors. But practically, achieving a goal like this is not possible for a large dataset. This code gives a basic idea of the working of a gradient descent algorithm on a toy dataset. It can be replaced with a dataset with of ones choice. The number of iterations are set to 5. Number of iterations can be toggled to achieve better results.
